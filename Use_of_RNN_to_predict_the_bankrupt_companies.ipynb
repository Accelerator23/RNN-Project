{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "gpuClass": "premium"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "premium",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Import the necessary Packages**\n"
      ],
      "metadata": {
        "id": "-CKd4rEjT9Sw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install fredapi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EDsK3YV-U9VK",
        "outputId": "ad76993e-540a-466f-f5ea-6a191c13a116"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting fredapi\n",
            "  Downloading fredapi-0.5.0-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from fredapi) (1.5.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->fredapi) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->fredapi) (2022.7.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas->fredapi) (1.22.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->fredapi) (1.16.0)\n",
            "Installing collected packages: fredapi\n",
            "Successfully installed fredapi-0.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "48DJbT7BL5pk",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Packages Imported \n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow.keras as keras\n",
        "from keras.layers import ELU, PReLU, LeakyReLU\n",
        "from keras.utils.vis_utils import plot_model, model_to_dot\n",
        "\n",
        "from google.colab import drive\n",
        "from google.colab import files\n",
        "\n",
        "from sklearn import preprocessing\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.utils import class_weight\n",
        "from sklearn.metrics import f1_score, roc_curve, auc\n",
        "from sklearn import metrics\n",
        "import seaborn as sns\n",
        "import datetime as dt\n",
        "from fredapi import Fred\n",
        "import os\n",
        "import requests\n",
        "from pprint import pprint\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step I: Load the data from pre-downloaded Excels**"
      ],
      "metadata": {
        "id": "2oVE6jrNUJtU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Data Obtain\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "#path of data in google drive\n",
        "pathbc = '/content/drive/MyDrive/Colab Notebooks/AI Data/BankruptedCompanies.csv'\n",
        "pathhc1 = '/content/drive/MyDrive/Colab Notebooks/AI Data/HealthyCompanies1.csv'\n",
        "pathhc2 = '/content/drive/MyDrive/Colab Notebooks/AI Data/HealthyCompanies2.csv'\n",
        "\n",
        "#1. bankrupted companies data\n",
        "bc_data = pd.read_csv(pathbc)\n",
        "bc_data = bc_data.reset_index(drop=True)\n",
        "bc_data.columns = [\"Years/Attributions\", \"Year 5\", \"Year 4\",\"Year 3\",\"Year 2\",\"Year 1\"]\n",
        "# change the antichronogical data into chronogical data by reversing the columns\n",
        "bc_data = bc_data[bc_data.columns[::-1]]\n",
        "bc_data.set_index(\"Years/Attributions\", inplace = True)\n",
        "\n",
        "#2. healthy companies data\n",
        "#data1\n",
        "hc_data1 = pd.read_csv(pathhc1)\n",
        "hc_data1 = hc_data1.reset_index(drop=True)\n",
        "hc_data1.columns = [\"Years/Attributions\", \"Year 5\", \"Year 4\",\"Year 3\",\"Year 2\",\"Year 1\"]\n",
        "#change the antichronogical data into chronogical data by reversing the columns\n",
        "hc_data1 = hc_data1[hc_data1.columns[::-1]] \n",
        "#data2\n",
        "hc_data2 = pd.read_csv(pathhc2)\n",
        "hc_data2 = hc_data2.reset_index(drop=True)\n",
        "hc_data2.columns = [\"Years/Attributions\", \"Year 5\", \"Year 4\",\"Year 3\",\"Year 2\",\"Year 1\"]\n",
        "# change the antichronogical data into chronogical data by reversing the columns\n",
        "hc_data2 = hc_data2[hc_data2.columns[::-1]]\n",
        "#concatenate\n",
        "hc_data = pd.concat([hc_data1,hc_data2], ignore_index=True)\n",
        "hc_data.set_index(\"Years/Attributions\", inplace = True)"
      ],
      "metadata": {
        "id": "kz73WuDaL8hV",
        "cellView": "form"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Basic data conversion\n",
        "\n",
        "#convert both data into numpy array\n",
        "bc_data1 = bc_data.iloc[:,0:5].to_numpy()\n",
        "hc_data3 = hc_data.iloc[:,0:5].to_numpy()\n",
        "#split the data into blocks with each block possesses 28 attributes\n",
        "bc_data2 = np.array(np.split(bc_data1.T,396,axis=1))\n",
        "hc_data4 = np.array(np.split(hc_data3.T,39140,axis=1))"
      ],
      "metadata": {
        "id": "DTw1MCf6L8fm",
        "cellView": "form"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Necessay Function Needed \n",
        "\n",
        "def confusionmatrixplot(confusionmatrix): #plot the confusion matrix given the array of data\n",
        "  cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusionmatrix, display_labels = [False, True])\n",
        "  cm_display.plot()\n",
        "  plt.show()\n",
        "\n",
        "def outlierremover(data,threshold): #remove the outliers with the specified threshold of our data\n",
        "  mu = np.mean(data.T.reshape(data.shape[2],data.shape[0]*5),axis= 1)\n",
        "  std = np.std(data.T.reshape(data.shape[2],data.shape[0]*5),axis= 1)\n",
        "  z = (data - mu) / std\n",
        "  mask = (z >= - threshold) & (z <= threshold)\n",
        "  clean_data = data[np.all(mask, axis = (1,2))]\n",
        "  return clean_data"
      ],
      "metadata": {
        "id": "8O9bKDbhfE-F",
        "cellView": "form"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step II: Our way to extract those data through API key**"
      ],
      "metadata": {
        "id": "XNipfvFK6VD2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Extract US Market data \n",
        "#Get the US market data\n",
        "# Set your FRED API key\n",
        "api_key = 'a28dbdf390abae7716f8916162008096'\n",
        "\n",
        "# Initialize the FRED API client\n",
        "fred = Fred(api_key=api_key)\n",
        "\n",
        "# Define the date range\n",
        "end_date = dt.date(2022, 12, 31)\n",
        "start_date = end_date - dt.timedelta(days=6*365)\n",
        "\n",
        "# Download real GDP growth (annual, percent change) data\n",
        "real_gdp_growth_data = fred.get_series('GDPC1', start_date, end_date)\n",
        "rm = np.mean(np.split(np.array(real_gdp_growth_data),6),axis = 1)\n",
        "real_gdp_growth_data = np.diff(rm) / rm[1:] * 100\n",
        "\n",
        "# Download 10-year Treasury Bill rate data (risk-free rate proxy)\n",
        "risk_free_rate_data = fred.get_series('GS10', start_date, end_date)\n",
        "risk_free_rate_data = np.array(risk_free_rate_data.resample('Y').mean())[1:]\n",
        "\n",
        "# Download unemployment rate (annual average) data\n",
        "unemployment_rate_data = fred.get_series('UNRATE', start_date, end_date)\n",
        "unemployment_rate_data = np.array(unemployment_rate_data.resample('Y').mean())[1:]\n",
        "\n",
        "# Download inflation rate (annual, percent change) data using Consumer Price Index for All Urban Consumers (CPI-U)\n",
        "inflation_data = fred.get_series('CPIAUCSL', start_date, end_date)\n",
        "# inflation_data = inflation_data.pct_change().resample('Y').mean() *100\n",
        "ri = np.mean(np.split(np.array(inflation_data),6),axis = 1)\n",
        "inflation_data = np.diff(ri) / ri[1:] * 100\n",
        "\n",
        "# Combine data into a single DataFrame\n",
        "market_data = pd.DataFrame(columns = ['2018', '2019','2020','2021','2022'], index = ['Real GDP Growth','Inflation','Unemployment','10-y Risk Free'])\n",
        "market_data.loc['Real GDP Growth'] = real_gdp_growth_data/100\n",
        "market_data.loc['Inflation'] = inflation_data/100\n",
        "market_data.loc['Unemployment'] = unemployment_rate_data/100\n",
        "market_data.loc['10-y Risk Free'] = risk_free_rate_data/100\n",
        "market_data"
      ],
      "metadata": {
        "id": "FsItszLf5l2S",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Extract fundamental data of corporations \n",
        "\n",
        "# Your Alpha Vantage API key\n",
        "api_key =\"VEUMZ77Y3NO13NKJ\"\n",
        "\n",
        "# companies' stock ticker\n",
        "companies = [\"IBM\"] # change here\n",
        "\n",
        "# API endpoints\n",
        "base_url = \"https://www.alphavantage.co/query?\"\n",
        "balance_sheet_url = f\"{base_url}function=BALANCE_SHEET&apikey={api_key}\"\n",
        "income_statement_url = f\"{base_url}function=INCOME_STATEMENT&apikey={api_key}\"\n",
        "stock_data_url = f\"{base_url}function=TIME_SERIES_DAILY_ADJUSTED&outputsize=full&apikey={api_key}\"\n",
        "\n",
        "# Fetch and print the fundamental data for each company\n",
        "try_data = pd.DataFrame()\n",
        "for symbol in companies:\n",
        "    print(f\"Fetching data for {symbol}...\")\n",
        "    balance_sheet = requests.get(f\"{balance_sheet_url}&symbol={symbol}\").json()\n",
        "    income_statement = requests.get(f\"{income_statement_url}&symbol={symbol}\").json()\n",
        "    stock_data =  requests.get(f\"{stock_data_url}&symbol={symbol}\").json()\n",
        "\n",
        "    data = balance_sheet['annualReports']\n",
        "    bs = pd.DataFrame(data)\n",
        "    bs.set_index('fiscalDateEnding', inplace=True)\n",
        "    data1 = income_statement['annualReports']\n",
        "    pl = pd.DataFrame(data1)\n",
        "    pl.set_index('fiscalDateEnding', inplace=True)\n",
        "    stock_data_df = pd.DataFrame(stock_data[\"Time Series (Daily)\"]).T\n",
        "    stock_data_df.index = pd.to_datetime(stock_data_df.index)\n",
        "    stock_data_df[\"5. adjusted close\"] = stock_data_df[\"5. adjusted close\"].astype(float)\n",
        "    stock_data_df[\"daily_returns\"] = stock_data_df[\"5. adjusted close\"].pct_change()\n",
        "\n",
        "    # Filter stock data for the last 5 years\n",
        "    five_years_ago = pd.Timestamp.now() - pd.DateOffset(years=6)\n",
        "    filtered_stock_data_df = stock_data_df[stock_data_df.index >= five_years_ago]\n",
        "    volatility = filtered_stock_data_df[\"daily_returns\"].rolling(window=90).std() * np.sqrt(90)\n",
        "    filtered_stock_data_df[\"90_day_volatility\"] = volatility\n",
        "    #Extract and compute all the 28 attributes from the financial statements\n",
        "    yty_stock_price = filtered_stock_data_df['5. adjusted close'].resample('Y').last().astype('float')[:-1].pct_change()[1:].to_numpy()\n",
        "    market_cap = bs['commonStockSharesOutstanding'].to_numpy().astype('float')*filtered_stock_data_df['5. adjusted close'].resample('Y').last().astype('float')[:-1][1:][::-1].to_numpy() / 1000000\n",
        "    ninty_day_stock_std = filtered_stock_data_df['90_day_volatility'].resample('Y').last()[1:-1].astype('float')\n",
        "    trading_volumn = filtered_stock_data_df['6. volume'].resample('Y').last().astype('float')[:-1][1:].to_numpy() / 1000\n",
        "    total_assets = bs['totalAssets'].to_numpy().astype('float') / 1000000\n",
        "    diluted_eps = pl['netIncome'].to_numpy().astype('float') / bs['totalShareholderEquity'].to_numpy().astype('float')\n",
        "    liab_over_assets = bs['totalLiabilities'].to_numpy().astype('float') / bs['totalAssets'].to_numpy().astype('float')\n",
        "    equity_over_assets = bs['totalShareholderEquity'].to_numpy().astype('float') / bs['totalAssets'].to_numpy().astype('float')\n",
        "    cash_over_assets = bs['cashAndCashEquivalentsAtCarryingValue'].to_numpy().astype('float') / bs['totalAssets'].to_numpy().astype('float')\n",
        "    current_ratio = bs['totalCurrentAssets'].to_numpy().astype('float') / bs['totalCurrentLiabilities'].to_numpy().astype('float')\n",
        "    inventory_over_sales = bs['inventory'].to_numpy().astype('float') / pl['totalRevenue'].to_numpy().astype('float')\n",
        "    netdebt_over_ebitda = (bs['longTermDebt'].to_numpy().astype('float') + bs['shortTermDebt'].to_numpy().astype('float') - bs['cashAndCashEquivalentsAtCarryingValue'].to_numpy().astype('float'))  / pl['ebitda'].to_numpy().astype('float')\n",
        "    ap_over_sales = bs['currentAccountsPayable'].to_numpy().astype('float') / pl['totalRevenue'].to_numpy().astype('float')\n",
        "    ar_over_sales = bs['currentNetReceivables'].to_numpy().astype('float') / pl['totalRevenue'].to_numpy().astype('float')\n",
        "    roe = pl['netIncome'].to_numpy().astype('float') / bs['totalShareholderEquity'].to_numpy().astype('float')\n",
        "    roa = pl['netIncome'].to_numpy().astype('float') / bs['totalAssets'].to_numpy().astype('float')\n",
        "    sales_over_assets = pl['totalRevenue'].to_numpy().astype('float') / bs['totalAssets'].to_numpy().astype('float')\n",
        "    net_income_margin = pl['netIncome'].to_numpy().astype('float') / pl['totalRevenue'].to_numpy().astype('float')\n",
        "    ebit_over_sales = pl['ebit'].to_numpy().astype('float') / pl['totalRevenue'].to_numpy().astype('float')\n",
        "    ebitda_over_sales = pl['ebitda'].to_numpy().astype('float') / pl['totalRevenue'].to_numpy().astype('float')\n",
        "    ebitda_over_assets = pl['ebitda'].to_numpy().astype('float') / bs['totalCurrentAssets'].to_numpy().astype('float')\n",
        "    debt_over_ebitda = (bs['longTermDebt'].to_numpy().astype('float') + bs['shortTermDebt'].to_numpy().astype('float')) / pl['ebitda'].to_numpy().astype('float')\n",
        "    wc_over_assets = (bs['totalCurrentAssets'].to_numpy().astype('float') - bs['totalCurrentLiabilities'].to_numpy().astype('float')) / bs['totalAssets'].to_numpy().astype('float')\n",
        "    price_to_book = filtered_stock_data_df['5. adjusted close'].resample('Y').last().astype('float')[:-1][1:].to_numpy()[::-1] / (bs['totalShareholderEquity'].to_numpy().astype('float') / bs['commonStockSharesOutstanding'].to_numpy().astype('float'))\n",
        "    raw = np.concatenate((yty_stock_price,market_cap,ninty_day_stock_std,trading_volumn,total_assets,diluted_eps,liab_over_assets,equity_over_assets,cash_over_assets,current_ratio,inventory_over_sales,netdebt_over_ebitda,ap_over_sales,ar_over_sales,roe,roa,sales_over_assets,net_income_margin,ebit_over_sales,ebitda_over_sales,ebitda_over_assets,debt_over_ebitda,wc_over_assets,price_to_book))\n",
        "    combined = np.split(raw,24)\n"
      ],
      "metadata": {
        "id": "mv4UXLtt5u9F",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Combine Data and Download as Excel\n",
        "\n",
        "combined_data = pd.DataFrame(combined,columns = ['2022','2021','2020','2019','2018'])\n",
        "combined_data = combined_data[combined_data.columns[::-1]]\n",
        "whole_data = pd.concat([combined_data,market_data])\n",
        "try_data = pd.concat([try_data,whole_data])\n",
        "try_data.to_csv('outputUSCELLULAR.csv', encoding = 'utf-8-sig') \n",
        "files.download('outputUSCELLULAR.csv')"
      ],
      "metadata": {
        "cellView": "form",
        "id": "fdzS_B5tifd9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step III: Conceptual Model and its performance**"
      ],
      "metadata": {
        "id": "v3k5fo7e5Zfy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Conceptual Model test\n",
        "b = 500\n",
        "accuracy = np.zeros((1,b))\n",
        "f1 = np.zeros((1,b))\n",
        "hcpower = np.zeros((1,b))\n",
        "bcpower = np.zeros((1,b))\n",
        "confusion = np.zeros((1,2,2))\n",
        "for i in range(b):\n",
        "    #randomly shuffle both datasets\n",
        "    np.random.shuffle(bc_data2)\n",
        "    np.random.shuffle(hc_data4)\n",
        "    #Separating into training, validating and testing data sets:\n",
        "    num_bc=396\n",
        "    num_hc=39140\n",
        "    #Define the train and test numbers of bc data\n",
        "    n_train_bc = 220\n",
        "    n_val_bc = 76\n",
        "    n_test_bc = 100\n",
        "    #Define the train and test numbers of hc data\n",
        "    n_val_hc = int((n_val_bc*1))\n",
        "    n_test_hc = int((n_test_bc*1))\n",
        "    n_train_hc = num_bc - n_val_hc - n_test_hc\n",
        "    #First select the test and validation dataset and keep it free from outlier remover(real-world senario)\n",
        "    test_tuples = (list(zip(bc_data2[0:n_test_bc],np.ones(n_test_bc))) \n",
        "            + list(zip(hc_data4[0:n_test_hc],np.zeros(n_test_hc))))\n",
        "    np.random.shuffle(test_tuples) #To mix the bankrupted and healty companies\n",
        "    val_tuples = (list(zip(bc_data2[n_test_bc:n_test_bc+n_val_bc],np.ones(n_val_bc))) \n",
        "            + list(zip(hc_data4[n_test_hc:n_test_hc+n_val_hc],np.zeros(n_val_hc))))\n",
        "    np.random.shuffle(val_tuples)\n",
        "    #Then remove the outliers of the both bc and hc datasets and form the traning datasets\n",
        "    bc_new = outlierremover(bc_data2[n_test_bc+n_val_bc:],threshold = 2000)\n",
        "    hc_new = outlierremover(hc_data4[n_test_bc+n_val_bc:],threshold = 4.5)\n",
        "    train_tuples = (list(zip(bc_new,np.ones(bc_new.shape[0]))) \n",
        "            + list(zip(hc_new,np.zeros(hc_new.shape[0]))))\n",
        "    np.random.shuffle(train_tuples)\n",
        "    #Extract the train,test,val datasets as np array from tuples\n",
        "    (train_x, train_y) = [np.array(t).astype('float32') for t in zip(*train_tuples)]\n",
        "    (val_x, val_y) = [np.array(t).astype('float32') for t in zip(*val_tuples)]\n",
        "    (test_x, test_y) = [np.array(t).astype('float32') for t in zip(*test_tuples)]  \n",
        "    mu_train = np.mean(train_x.T.reshape(train_x.shape[2],train_x.shape[0]*5),axis= 1)\n",
        "    std_train = np.std(train_x.T.reshape(train_x.shape[2],train_x.shape[0]*5),axis= 1)\n",
        "    train_x = (train_x - mu_train) / std_train\n",
        "    test_x = (test_x - mu_train) / std_train\n",
        "    val_x = (val_x - mu_train) / std_train \n",
        "    #Define the weight of loss function to focus more on the bankrupt companies(since bankrupt data is way less than the health data)\n",
        "    class_weight_auto  = class_weight.compute_class_weight(\n",
        "                                          class_weight = \"balanced\",\n",
        "                                          classes = np.unique(train_y),\n",
        "                                          y = train_y)\n",
        "    class_weight_auto = {0:0.5, 1:120}\n",
        "    #Define the callback to prevent overfitting by tracking the movement of val_loss\n",
        "    es_callback = keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
        "    actual = test_y\n",
        "    METRICS = [\n",
        "    keras.metrics.TruePositives(name='tp'),\n",
        "    keras.metrics.FalsePositives(name='fp'),\n",
        "    keras.metrics.TrueNegatives(name='tn'),\n",
        "    keras.metrics.FalseNegatives(name='fn'),\n",
        "    keras.metrics.BinaryAccuracy(name='accuracy'),\n",
        "    keras.metrics.Recall(name='recall'),\n",
        "    tf.keras.metrics.Precision(name='precision')\n",
        "    ]\n",
        "    model = keras.Sequential()\n",
        "    model.add(keras.layers.LSTM(64, input_shape=(5,28), activation='tanh',dropout = 0.1))\n",
        "    model.add(keras.layers.Dense(8, activation='PReLU'))\n",
        "    model.add(keras.layers.Dropout(0.125))\n",
        "    model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
        "    model.compile(loss= tf.keras.losses.BinaryCrossentropy(), optimizer = tf.keras.optimizers.Adam(), metrics = METRICS)\n",
        "    model.fit(train_x, train_y, validation_data=(val_x, val_y),epochs = 40, batch_size = 1024, callbacks = [es_callback],class_weight=class_weight_auto,verbose = 1)\n",
        "    pre2 = np.rint(model.predict(test_x))\n",
        "    f12 = f1_score(actual, pre2, average = 'weighted')\n",
        "    confusion_matrix2 = metrics.confusion_matrix(actual, pre2)\n",
        "    confusion[0]+=confusion_matrix2\n",
        "    result1 = model.evaluate(test_x,test_y,batch_size=256,verbose = 1)\n",
        "    bcpower[0][i] = result1[1]/100\n",
        "    hcpower[0][i] = result1[3]/100\n",
        "    accuracy[0][i] = result1[5]\n",
        "    f1[0][i] = f12\n"
      ],
      "metadata": {
        "id": "6bHwH0CV5TET",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Conceptual Model Performance Table and Confusion Matrix\n",
        "\n",
        "accuracy_meanc = np.mean(accuracy,axis = 1)\n",
        "max_accc = np.max(accuracy,axis = 1)\n",
        "min_accc = np.min(accuracy,axis = 1)\n",
        "acc_stdc = np.std(accuracy,axis = 1)\n",
        "mean_bcpowerc = np.mean(bcpower,axis = 1)\n",
        "mean_hcpowerc = np.mean(hcpower,axis = 1)\n",
        "\n",
        "conceptual = pd.DataFrame(index = ['Conceptual LSTM'])\n",
        "conceptual['Accuracy'] = np.round(accuracy_meanc,4)\n",
        "conceptual['Recall BC'] = np.round(mean_bcpowerc,4)\n",
        "conceptual['Recall HC'] = np.round(mean_hcpowerc,4)\n",
        "conceptual['Max Accr'] = np.round(max_accc,4)\n",
        "conceptual['Min Accr'] = np.round(min_accc,4)\n",
        "conceptual['Accr Std'] = np.round(acc_stdc,4)\n",
        "\n",
        "print(conceptual)\n",
        "print(confusionmatrixplot(confusion[0]/b))"
      ],
      "metadata": {
        "cellView": "form",
        "id": "etAzysTXZclt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step IV: Best Prediction model filter, selection and result presentation**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "hHGsmZCEU8Y9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Prediction Model Filter 1 (10 models choose 3)\n",
        "\n",
        "a = 1000\n",
        "accuracy_store = np.zeros((10,a))\n",
        "f1_store = np.zeros((10,a))\n",
        "auc_store = np.zeros((10,a))\n",
        "for i in range(a):\n",
        "    np.random.shuffle(bc_data2)\n",
        "    np.random.shuffle(hc_data4)\n",
        "    #Separating into training, validating and testing data sets:\n",
        "    proportion_train = 0.8\n",
        "    proportion_val = 0.1\n",
        "    num_bc=396\n",
        "    num_hc=39140\n",
        "    n_train_bc = 220\n",
        "    n_val_bc = 76\n",
        "    n_test_bc = 100\n",
        "    n_val_hc = int((n_val_bc*1))\n",
        "    n_test_hc = int((n_test_bc*1))\n",
        "    n_train_hc = num_bc - n_val_hc - n_test_hc\n",
        "    test_tuples = (list(zip(bc_data2[0:n_test_bc],np.ones(n_test_bc))) \n",
        "            + list(zip(hc_data4[0:n_test_hc],np.zeros(n_test_hc))))\n",
        "    np.random.shuffle(test_tuples) #To mix the bankrupted and healty companies\n",
        "    val_tuples = (list(zip(bc_data2[n_test_bc:n_test_bc+n_val_bc],np.ones(n_val_bc))) \n",
        "            + list(zip(hc_data4[n_test_hc:n_test_hc+n_val_hc],np.zeros(n_val_hc))))\n",
        "    np.random.shuffle(val_tuples)\n",
        "\n",
        "  #Then remove the outliers of the both bc and hc datasets\n",
        "    def outlierremover(data,threshold):\n",
        "      mu = np.mean(data.T.reshape(data.shape[2],data.shape[0]*5),axis= 1)\n",
        "      std = np.std(data.T.reshape(data.shape[2],data.shape[0]*5),axis= 1)\n",
        "      z = (data - mu) / std\n",
        "      mask = (z >= - threshold) & (z <= threshold)\n",
        "      clean_data = data[np.all(mask, axis = (1,2))]\n",
        "      return clean_data\n",
        "    bc_new = outlierremover(bc_data2[n_test_bc+n_val_bc:],threshold = 2000)\n",
        "    hc_new = outlierremover(hc_data4[n_test_bc+n_val_bc:],threshold = 5)\n",
        "    train_tuples = (list(zip(bc_new,np.ones(bc_new.shape[0]))) \n",
        "            + list(zip(hc_new,np.zeros(hc_new.shape[0]))))\n",
        "    np.random.shuffle(train_tuples)\n",
        "\n",
        "    (train_x, train_y) = [np.array(t).astype('float32') for t in zip(*train_tuples)]\n",
        "    (val_x, val_y) = [np.array(t).astype('float32') for t in zip(*val_tuples)]\n",
        "    (test_x, test_y) = [np.array(t).astype('float32') for t in zip(*test_tuples)]  \n",
        "    mu_train = np.mean(train_x.T.reshape(train_x.shape[2],train_x.shape[0]*5),axis= 1)\n",
        "    std_train = np.std(train_x.T.reshape(train_x.shape[2],train_x.shape[0]*5),axis= 1)\n",
        "    train_x = (train_x - mu_train) / std_train\n",
        "    test_x = (test_x - mu_train) / std_train\n",
        "    val_x = (val_x - mu_train) / std_train \n",
        "    class_weight_auto  = class_weight.compute_class_weight(\n",
        "                                          class_weight = \"balanced\",\n",
        "                                          classes = np.unique(train_y),\n",
        "                                          y = train_y)\n",
        "    class_weight_auto = dict(enumerate(class_weight_auto))\n",
        "    es_callback = keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
        "\n",
        "    #model1 GRU\n",
        "    model = keras.Sequential()\n",
        "    model.add(keras.layers.GRU(32,input_shape=(5,train_x.shape[2]),activation = 'tanh', dropout = 0.125,return_sequences=False,kernel_regularizer = keras.regularizers.L1(0.00007)))\n",
        "    model.add(keras.layers.Dense(16, activation = 'PReLU'))\n",
        "    model.add(keras.layers.Dropout(0.125))\n",
        "    model.add(keras.layers.Dense(4, activation = 'PReLU'))\n",
        "    model.add(keras.layers.Dropout(0.125))\n",
        "    model.add(keras.layers.Dense(1,activation = 'sigmoid')) \n",
        "    model.compile(loss= tf.keras.losses.BinaryCrossentropy(), optimizer = tf.keras.optimizers.Adam(), metrics = ['accuracy'])\n",
        "    try1 = model.fit(train_x, train_y, validation_data=(val_x, val_y),epochs = 40, batch_size = 1024, callbacks = [es_callback],class_weight=class_weight_auto,verbose = 0)\n",
        "    test_loss, test_acc = model.evaluate(test_x,test_y,batch_size=256)\n",
        "    pre = model.predict(test_x)\n",
        "    predicted = np.rint(model.predict(test_x))\n",
        "    actual = test_y\n",
        "    f1 = f1_score(actual, predicted, average = 'weighted')\n",
        "    fpr,tpr,threshold = roc_curve(test_y, pre)\n",
        "    roc_auc = auc(fpr,tpr)\n",
        "    accuracy_store[0][i] = test_acc\n",
        "    f1_store[0][i] = f1\n",
        "    auc_store[0][i] = roc_auc\n",
        "\n",
        "    #model2 SimpleRNN\n",
        "    model = keras.Sequential()\n",
        "    model.add(keras.layers.SimpleRNN(128,input_shape=(5,train_x.shape[2]),activation = 'tanh', dropout = 0.125))\n",
        "    model.add(keras.layers.Dense(32, activation = 'PReLU'))\n",
        "    model.add(keras.layers.Dropout(0.125))\n",
        "    model.add(keras.layers.Dense(1,activation = 'sigmoid')) \n",
        "    model.compile(loss= tf.keras.losses.BinaryCrossentropy(), optimizer = tf.keras.optimizers.Adam(), metrics = ['accuracy'])\n",
        "    try1 = model.fit(train_x, train_y, validation_data=(val_x, val_y),epochs = 40, batch_size = 1024, callbacks = [es_callback],class_weight=class_weight_auto,verbose = 0)\n",
        "    test_loss, test_acc = model.evaluate(test_x,test_y,batch_size=256)\n",
        "    pre = model.predict(test_x)\n",
        "    predicted = np.rint(model.predict(test_x))\n",
        "    actual = test_y\n",
        "    f1 = f1_score(actual, predicted, average = 'weighted')\n",
        "    fpr,tpr,threshold = roc_curve(test_y, pre)\n",
        "    roc_auc = auc(fpr,tpr)\n",
        "    accuracy_store[1][i] = test_acc\n",
        "    f1_store[1][i] = f1\n",
        "    auc_store[1][i] = roc_auc\n",
        "\n",
        "    #model3 LSTM\n",
        "    model = keras.Sequential()\n",
        "    model.add(keras.layers.LSTM(64,input_shape=(5,train_x.shape[2]),activation = 'tanh', dropout = 0.125))\n",
        "    model.add(keras.layers.Dense(32, activation = 'PReLU'))\n",
        "    model.add(keras.layers.Dropout(0.125))\n",
        "    model.add(keras.layers.Dense(1,activation = 'sigmoid')) \n",
        "    model.compile(loss= tf.keras.losses.BinaryCrossentropy(), optimizer = tf.keras.optimizers.Adam(), metrics = ['accuracy'])\n",
        "    try1 = model.fit(train_x, train_y, validation_data=(val_x, val_y),epochs = 40, batch_size = 1024, callbacks = [es_callback],class_weight=class_weight_auto,verbose = 0)\n",
        "    test_loss, test_acc = model.evaluate(test_x,test_y,batch_size=256)\n",
        "    pre = model.predict(test_x)\n",
        "    predicted = np.rint(model.predict(test_x))\n",
        "    actual = test_y\n",
        "    f1 = f1_score(actual, predicted, average = 'weighted')\n",
        "    fpr,tpr,threshold = roc_curve(test_y, pre)\n",
        "    roc_auc = auc(fpr,tpr)\n",
        "    accuracy_store[2][i] = test_acc\n",
        "    f1_store[2][i] = f1\n",
        "    auc_store[2][i] = roc_auc\n",
        "\n",
        "    #model4 Dense\n",
        "    model = keras.Sequential()\n",
        "    model.add(keras.layers.Flatten())\n",
        "    model.add(keras.layers.Dense(128,input_shape=(train_x.shape[2],),activation = 'tanh'))\n",
        "    model.add(keras.layers.Dropout(0.125))\n",
        "    model.add(keras.layers.Dense(64,activation = 'PReLU'))\n",
        "    model.add(keras.layers.Dropout(0.125))\n",
        "    model.add(keras.layers.Dense(32, activation = 'PReLU'))\n",
        "    model.add(keras.layers.Dropout(0.125))\n",
        "    model.add(keras.layers.Dense(1,activation = 'sigmoid')) \n",
        "    model.compile(loss= tf.keras.losses.BinaryCrossentropy(), optimizer = tf.keras.optimizers.Adam(), metrics = ['accuracy'])\n",
        "    try1 = model.fit(train_x, train_y, validation_data=(val_x, val_y),epochs = 40, batch_size = 1024, callbacks = [es_callback],class_weight=class_weight_auto,verbose = 0)\n",
        "    test_loss, test_acc = model.evaluate(test_x,test_y,batch_size=256)\n",
        "    pre = model.predict(test_x)\n",
        "    predicted = np.rint(model.predict(test_x))\n",
        "    actual = test_y\n",
        "    f1 = f1_score(actual, predicted, average = 'weighted')\n",
        "    fpr,tpr,threshold = roc_curve(test_y, pre)\n",
        "    roc_auc = auc(fpr,tpr)\n",
        "    accuracy_store[3][i] = test_acc\n",
        "    f1_store[3][i] = f1\n",
        "    auc_store[3][i] = roc_auc\n",
        "\n",
        "    #model5 Conv-1D-v\n",
        "    model = keras.Sequential()\n",
        "    model.add(keras.layers.Conv1D(filters = 128,kernel_size = 5,input_shape=(28,5),activation = 'tanh',padding = 'same',kernel_regularizer = keras.regularizers.L1(0.00007)))\n",
        "    model.add(keras.layers.Dropout(0.125))\n",
        "    model.add(keras.layers.MaxPooling1D(pool_size = 2))\n",
        "    model.add(keras.layers.Conv1D(filters = 64,kernel_size = 4,activation = 'tanh',padding = 'same',kernel_regularizer = keras.regularizers.L2(0.00007)))\n",
        "    model.add(keras.layers.Dropout(0.125))\n",
        "    model.add(keras.layers.MaxPooling1D(pool_size = 2))\n",
        "    model.add(keras.layers.Flatten())\n",
        "    model.add(keras.layers.Dense(32, activation = 'PReLU'))\n",
        "    model.add(keras.layers.Dropout(0.125))\n",
        "    model.add(keras.layers.Dense(4, activation = 'PReLU'))\n",
        "    model.add(keras.layers.Dropout(0.125))\n",
        "    model.add(keras.layers.Dense(1,activation = 'sigmoid'))\n",
        "    model.compile(loss= tf.keras.losses.BinaryCrossentropy(), optimizer = tf.keras.optimizers.Adam(), metrics = ['accuracy'])\n",
        "    try1 = model.fit(train_x.transpose(0,2,1), train_y, validation_data=(val_x.transpose(0,2,1), val_y),epochs = 40, batch_size = 1024, callbacks = [es_callback],class_weight=class_weight_auto,verbose = 0)\n",
        "    test_loss, test_acc = model.evaluate(test_x.transpose(0,2,1),test_y,batch_size=256)\n",
        "    pre = model.predict(test_x.transpose(0,2,1))\n",
        "    predicted = np.rint(model.predict(test_x.transpose(0,2,1)))\n",
        "    actual = test_y\n",
        "    f1 = f1_score(actual, predicted, average = 'weighted')\n",
        "    fpr,tpr,threshold = roc_curve(test_y, pre)\n",
        "    roc_auc = auc(fpr,tpr)\n",
        "    accuracy_store[4][i] = test_acc\n",
        "    f1_store[4][i] = f1\n",
        "    auc_store[4][i] = roc_auc\n",
        "\n",
        "    #model6 Conv-1D-h\n",
        "    model = keras.Sequential()\n",
        "    model.add(keras.layers.Conv1D(filters = 256,kernel_size = 5,input_shape=(5,28),activation = 'tanh',padding = 'same'))\n",
        "    model.add(keras.layers.Dropout(0.125))\n",
        "    model.add(keras.layers.MaxPooling1D(pool_size = 2))\n",
        "    model.add(keras.layers.Flatten())\n",
        "    model.add(keras.layers.Dense(128, activation = 'PReLU'))\n",
        "    model.add(keras.layers.Dropout(0.125))\n",
        "    model.add(keras.layers.Dense(1,activation = 'sigmoid'))\n",
        "    model.compile(loss= tf.keras.losses.BinaryCrossentropy(), optimizer = tf.keras.optimizers.Adam(), metrics = ['accuracy'])\n",
        "    try1 = model.fit(train_x, train_y, validation_data=(val_x, val_y),epochs = 40, batch_size = 1024, callbacks = [es_callback],class_weight=class_weight_auto,verbose = 0)\n",
        "    test_loss, test_acc = model.evaluate(test_x,test_y,batch_size=256)\n",
        "    pre = model.predict(test_x)\n",
        "    predicted = np.rint(model.predict(test_x))\n",
        "    actual = test_y\n",
        "    f1 = f1_score(actual, predicted, average = 'weighted')\n",
        "    fpr,tpr,threshold = roc_curve(test_y, pre)\n",
        "    roc_auc = auc(fpr,tpr)\n",
        "    accuracy_store[5][i] = test_acc\n",
        "    f1_store[5][i] = f1\n",
        "    auc_store[5][i] = roc_auc\n",
        "\n",
        "    #model7 BI-LSTM\n",
        "    model = keras.Sequential()\n",
        "    model.add(keras.layers.Bidirectional(keras.layers.LSTM(50,input_shape=(5,train_x.shape[2]),activation = 'tanh', dropout = 0.1)))\n",
        "    model.add(keras.layers.Dense(25, activation = 'PReLU'))\n",
        "    model.add(keras.layers.Dropout(0.2))\n",
        "    model.add(keras.layers.Dense(5, activation = 'PReLU'))\n",
        "    model.add(keras.layers.Dropout(0.2))\n",
        "    model.add(keras.layers.Dense(1,activation = 'sigmoid')) \n",
        "    model.compile(loss= tf.keras.losses.BinaryCrossentropy(), optimizer = tf.keras.optimizers.Adam(), metrics = ['accuracy'])\n",
        "    try1 = model.fit(train_x, train_y, validation_data=(val_x, val_y),epochs = 40, batch_size = 1024, callbacks = [es_callback],class_weight=class_weight_auto,verbose = 0)\n",
        "    test_loss, test_acc = model.evaluate(test_x,test_y,batch_size=256)\n",
        "    pre = model.predict(test_x)\n",
        "    predicted = np.rint(model.predict(test_x))\n",
        "    actual = test_y\n",
        "    f1 = f1_score(actual, predicted, average = 'weighted')\n",
        "    fpr,tpr,threshold = roc_curve(test_y, pre)\n",
        "    roc_auc = auc(fpr,tpr)\n",
        "    accuracy_store[6][i] = test_acc\n",
        "    f1_store[6][i] = f1\n",
        "    auc_store[6][i] = roc_auc\n",
        "\n",
        "    #model8 Bi-GRU\n",
        "    model = keras.Sequential()\n",
        "    model.add(keras.layers.Bidirectional(keras.layers.GRU(64,input_shape=(5,train_x.shape[2]),activation = 'tanh', dropout = 0.125)))\n",
        "    model.add(keras.layers.Dense(32, activation = 'PReLU'))\n",
        "    model.add(keras.layers.Dropout(0.125))\n",
        "    model.add(keras.layers.Dense(1,activation = 'sigmoid')) \n",
        "    model.compile(loss= tf.keras.losses.BinaryCrossentropy(), optimizer = tf.keras.optimizers.Adam(), metrics = ['accuracy'])\n",
        "    try1 = model.fit(train_x, train_y, validation_data=(val_x, val_y),epochs = 40, batch_size = 1024, callbacks = [es_callback],class_weight=class_weight_auto,verbose = 0)\n",
        "    test_loss, test_acc = model.evaluate(test_x,test_y,batch_size=256)\n",
        "    pre = model.predict(test_x)\n",
        "    predicted = np.rint(model.predict(test_x))\n",
        "    actual = test_y\n",
        "    f1 = f1_score(actual, predicted, average = 'weighted')\n",
        "    fpr,tpr,threshold = roc_curve(test_y, pre)\n",
        "    roc_auc = auc(fpr,tpr)\n",
        "    accuracy_store[7][i] = test_acc\n",
        "    f1_store[7][i] = f1\n",
        "    auc_store[7][i] = roc_auc\n",
        "\n",
        "    #model 9 Conv lstm 1d\n",
        "    model = keras.Sequential()\n",
        "    model.add(keras.layers.Conv1D(filters = 64,kernel_size = 5,input_shape=(5,28),activation = 'tanh',padding = 'same'))\n",
        "    model.add(keras.layers.Dropout(0.125))\n",
        "    model.add(keras.layers.LSTM(128,activation = 'tanh', dropout = 0.25))\n",
        "    model.add(keras.layers.Dense(32, activation = 'PReLU'))\n",
        "    model.add(keras.layers.Dropout(0.125))\n",
        "    model.add(keras.layers.Dense(1,activation = 'sigmoid'))\n",
        "    model.compile(loss= tf.keras.losses.BinaryCrossentropy(), optimizer = tf.keras.optimizers.Adam(), metrics = ['accuracy'])\n",
        "    try1 = model.fit(train_x, train_y, validation_data=(val_x, val_y),epochs = 40, batch_size = 1024, callbacks = [es_callback],class_weight=class_weight_auto,verbose = 0)\n",
        "    test_loss, test_acc = model.evaluate(test_x,test_y,batch_size=256)\n",
        "    pre = model.predict(test_x)\n",
        "    predicted = np.rint(model.predict(test_x))\n",
        "    actual = test_y\n",
        "    f1 = f1_score(actual, predicted, average = 'weighted')\n",
        "    fpr,tpr,threshold = roc_curve(test_y, pre)\n",
        "    roc_auc = auc(fpr,tpr)\n",
        "    accuracy_store[8][i] = test_acc\n",
        "    f1_store[8][i] = f1\n",
        "    auc_store[8][i] = roc_auc\n",
        "  \n",
        "    #model 10 Locally connected 1d\n",
        "    model = keras.Sequential()\n",
        "    model.add(keras.layers.LocallyConnected1D(filters = 256,kernel_size = 5,input_shape=(5,28),activation = 'tanh',padding = 'valid'))\n",
        "    model.add(keras.layers.Dropout(0.125))\n",
        "    model.add(keras.layers.Flatten())\n",
        "    model.add(keras.layers.Dense(128, activation = 'PReLU'))\n",
        "    model.add(keras.layers.Dropout(0.125))\n",
        "    model.add(keras.layers.Dense(1,activation = 'sigmoid'))\n",
        "    model.compile(loss= tf.keras.losses.BinaryCrossentropy(), optimizer = tf.keras.optimizers.Adam(), metrics = ['accuracy'])\n",
        "    try1 = model.fit(train_x, train_y, validation_data=(val_x, val_y),epochs = 40, batch_size = 1024, callbacks = [es_callback],class_weight=class_weight_auto,verbose = 0)\n",
        "    test_loss, test_acc = model.evaluate(test_x,test_y,batch_size=256)\n",
        "    pre = model.predict(test_x)\n",
        "    predicted = np.rint(model.predict(test_x))\n",
        "    actual = test_y\n",
        "    f1 = f1_score(actual, predicted, average = 'weighted')\n",
        "    fpr,tpr,threshold = roc_curve(test_y, pre)\n",
        "    roc_auc = auc(fpr,tpr)\n",
        "    accuracy_store[9][i] = test_acc\n",
        "    f1_store[9][i] = f1\n",
        "    auc_store[9][i] = roc_auc"
      ],
      "metadata": {
        "cellView": "form",
        "id": "LwStJoIdSyPA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 10 Models Performance Comparison Table\n",
        "\n",
        "# result performance presentation\n",
        "auc_mean = np.mean(auc_store, axis = 1)\n",
        "accuracy_mean = np.mean(accuracy_store,axis = 1)\n",
        "f1_mean = np.mean(f1_store,axis = 1)\n",
        "max_acc = np.max(accuracy_store,axis = 1)\n",
        "min_acc = np.min(accuracy_store,axis = 1)\n",
        "acc_std = np.std(accuracy_store,axis = 1)\n",
        "max_f1 = np.max(f1_store,axis = 1)\n",
        "min_f1 = np.min(f1_store,axis = 1)\n",
        "max_auc = np.max(auc_store,axis = 1)\n",
        "min_auc = np.min(auc_store,axis = 1)\n",
        "\n",
        "compare = pd.DataFrame(index = ['GRU','SimpleRNN','LSTM','Dense','Conv 1D Vert','Conv 1D Hori','Bi-LSTM','Bi-GRU','Conv LSTM 1D','LocallyConv1D'])\n",
        "compare['Accuracy'] = np.round(accuracy_mean,4)\n",
        "compare['F1 Score'] = np.round(f1_mean,4)\n",
        "compare['AUC Value'] = np.round(auc_mean,4)\n",
        "compare['Max Accr'] = np.round(max_acc,4)\n",
        "compare['Min Accr'] = np.round(min_acc,4)\n",
        "compare['Accr Std'] = np.round(acc_std,4)\n",
        "compare"
      ],
      "metadata": {
        "cellView": "form",
        "id": "XAQLQieyXJ6d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Best Prediction Models selection and Performance presentations \n",
        "\n",
        "b = 1000\n",
        "accuracy = np.zeros((3,b))\n",
        "f1 = np.zeros((3,b))\n",
        "hcpower = np.zeros((3,b))\n",
        "bcpower = np.zeros((3,b))\n",
        "confusion = np.zeros((3,2,2))\n",
        "for i in range(b):\n",
        "    np.random.shuffle(bc_data2)\n",
        "    np.random.shuffle(hc_data4)\n",
        "    num_bc=396\n",
        "    num_hc=39140\n",
        "    n_train_bc = 220\n",
        "    n_val_bc = 76\n",
        "    n_test_bc = 100\n",
        "    n_val_hc = int((n_val_bc*1))\n",
        "    n_test_hc = int((n_test_bc*1))\n",
        "    n_train_hc = num_bc - n_val_hc - n_test_hc\n",
        "    test_tuples = (list(zip(bc_data2[0:n_test_bc],np.ones(n_test_bc))) \n",
        "            + list(zip(hc_data4[0:n_test_hc],np.zeros(n_test_hc))))\n",
        "    np.random.shuffle(test_tuples) \n",
        "    val_tuples = (list(zip(bc_data2[n_test_bc:n_test_bc+n_val_bc],np.ones(n_val_bc))) \n",
        "            + list(zip(hc_data4[n_test_hc:n_test_hc+n_val_hc],np.zeros(n_val_hc))))\n",
        "    np.random.shuffle(val_tuples)\n",
        "    bc_new = outlierremover(bc_data2[n_test_bc+n_val_bc:],threshold = 2000)\n",
        "    hc_new = outlierremover(hc_data4[n_test_bc+n_val_bc:],threshold = 4.5)\n",
        "    train_tuples = (list(zip(bc_new,np.ones(bc_new.shape[0]))) \n",
        "            + list(zip(hc_new,np.zeros(hc_new.shape[0]))))\n",
        "    np.random.shuffle(train_tuples)\n",
        "    (train_x, train_y) = [np.array(t).astype('float32') for t in zip(*train_tuples)]\n",
        "    (val_x, val_y) = [np.array(t).astype('float32') for t in zip(*val_tuples)]\n",
        "    (test_x, test_y) = [np.array(t).astype('float32') for t in zip(*test_tuples)]  \n",
        "    mu_train = np.mean(train_x.T.reshape(train_x.shape[2],train_x.shape[0]*5),axis= 1)\n",
        "    std_train = np.std(train_x.T.reshape(train_x.shape[2],train_x.shape[0]*5),axis= 1)\n",
        "    train_x = (train_x - mu_train) / std_train\n",
        "    test_x = (test_x - mu_train) / std_train\n",
        "    val_x = (val_x - mu_train) / std_train \n",
        "    # class_weight_auto  = class_weight.compute_class_weight(\n",
        "    #                                       class_weight = \"balanced\",\n",
        "    #                                       classes = np.unique(train_y),\n",
        "    #                                       y = train_y)\n",
        "    class_weight_auto = {0:0.5, 1:120}\n",
        "    es_callback = keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
        "    actual = test_y\n",
        "    METRICS = [\n",
        "    keras.metrics.TruePositives(name='tp'),\n",
        "    keras.metrics.FalsePositives(name='fp'),\n",
        "    keras.metrics.TrueNegatives(name='tn'),\n",
        "    keras.metrics.FalseNegatives(name='fn'),\n",
        "    keras.metrics.BinaryAccuracy(name='accuracy'),\n",
        "    keras.metrics.Recall(name='recall'),\n",
        "    tf.keras.metrics.Precision(name='precision')\n",
        "    ]\n",
        "\n",
        "    # #Final Model1: Conv 1D verticle Model\n",
        "    model = keras.Sequential()\n",
        "    model.add(keras.layers.Conv1D(filters = 128,kernel_size = 5,input_shape=(28,5),activation = 'tanh',padding = 'same',kernel_regularizer = keras.regularizers.L1(0.00007)))\n",
        "    model.add(keras.layers.Dropout(0.125))\n",
        "    model.add(keras.layers.MaxPooling1D(pool_size = 2))\n",
        "    model.add(keras.layers.Conv1D(filters = 64,kernel_size = 4,activation = 'tanh',padding = 'same',kernel_regularizer = keras.regularizers.L2(0.00007)))\n",
        "    model.add(keras.layers.Dropout(0.125))\n",
        "    model.add(keras.layers.MaxPooling1D(pool_size = 2))\n",
        "    model.add(keras.layers.Flatten())\n",
        "    model.add(keras.layers.Dense(32, activation = 'PReLU'))\n",
        "    model.add(keras.layers.Dropout(0.125))\n",
        "    model.add(keras.layers.Dense(4, activation = 'PReLU'))\n",
        "    model.add(keras.layers.Dropout(0.125))\n",
        "    model.add(keras.layers.Dense(1,activation = 'sigmoid'))\n",
        "    model.compile(loss= tf.keras.losses.BinaryCrossentropy(), optimizer = tf.keras.optimizers.Adam(), metrics = METRICS)\n",
        "    try1 = model.fit(train_x.transpose(0,2,1), train_y, validation_data=(val_x.transpose(0,2,1), val_y),epochs = 40, batch_size = 1024, callbacks = [es_callback],class_weight=class_weight_auto,verbose = 0)\n",
        "    pre1 = np.rint(model.predict(test_x.transpose(0,2,1)))\n",
        "    f11 = f1_score(actual, pre1, average = 'weighted')\n",
        "    confusion_matrix = metrics.confusion_matrix(actual, pre1)\n",
        "    confusion[0]+=confusion_matrix\n",
        "    result = model.evaluate(test_x.transpose(0,2,1),test_y,batch_size=256,verbose = 0)\n",
        "    bcpower[0][i] = result[1]/100\n",
        "    hcpower[0][i] = result[3]/100\n",
        "    accuracy[0][i] = result[5]\n",
        "    \n",
        "\n",
        "    #Final Model2: BI-LSTM\n",
        "    model1 = keras.Sequential()\n",
        "    model1.add(keras.layers.Bidirectional(keras.layers.LSTM(50,input_shape=(5,train_x.shape[2]),activation = 'tanh', dropout = 0.1,kernel_regularizer = keras.regularizers.L1(0.00007))))\n",
        "    model1.add(keras.layers.Dense(25, activation = 'PReLU'))\n",
        "    model1.add(keras.layers.Dropout(0.2))\n",
        "    model1.add(keras.layers.Dense(5, activation = 'PReLU'))\n",
        "    model1.add(keras.layers.Dropout(0.2))\n",
        "    model1.add(keras.layers.Dense(1,activation = 'sigmoid')) \n",
        "    model1.compile(loss= tf.keras.losses.BinaryCrossentropy(), optimizer = tf.keras.optimizers.Adam(), metrics = METRICS)\n",
        "    try2 = model1.fit(train_x, train_y, validation_data=(val_x, val_y),epochs = 40, batch_size = 1024, callbacks = [es_callback],class_weight=class_weight_auto,verbose = 0)\n",
        "    pre2 = np.rint(model1.predict(test_x))\n",
        "    f12 = f1_score(actual, pre2, average = 'weighted')\n",
        "    confusion_matrix2 = metrics.confusion_matrix(actual, pre2)\n",
        "    confusion[1]+=confusion_matrix2\n",
        "    result1 = model1.evaluate(test_x,test_y,batch_size=256,verbose = 0)\n",
        "    bcpower[1][i] = result1[1]/100\n",
        "    hcpower[1][i] = result1[3]/100\n",
        "    accuracy[1][i] = result1[5]\n",
        "\n",
        "    #Final Model3: GRU\n",
        "    model2 = keras.Sequential()\n",
        "    model2.add(keras.layers.GRU(32,input_shape=(5,train_x.shape[2]),activation = 'tanh', dropout = 0.125,return_sequences=False,kernel_regularizer = keras.regularizers.L1(0.00007)))\n",
        "    model2.add(keras.layers.Dense(16, activation = 'PReLU'))\n",
        "    model2.add(keras.layers.Dropout(0.125))\n",
        "    model2.add(keras.layers.Dense(4, activation = 'PReLU'))\n",
        "    model2.add(keras.layers.Dropout(0.125))\n",
        "    model2.add(keras.layers.Dense(1,activation = 'sigmoid')) \n",
        "    model2.compile(loss= tf.keras.losses.BinaryCrossentropy(), optimizer = tf.keras.optimizers.Adam(), metrics = METRICS)\n",
        "    try3 = model2.fit(train_x, train_y, validation_data=(val_x, val_y),epochs = 40, batch_size = 1024, callbacks = [es_callback],class_weight=class_weight_auto,verbose = 0)\n",
        "    pre3 = np.rint(model2.predict(test_x))\n",
        "    f13 = f1_score(actual, pre3, average = 'weighted')\n",
        "    confusion_matrix3 = metrics.confusion_matrix(actual, pre3)\n",
        "    confusion[2]+=confusion_matrix3\n",
        "    result2 = model2.evaluate(test_x,test_y,batch_size=256,verbose = 0)\n",
        "    bcpower[2][i] = result2[1]/100\n",
        "    hcpower[2][i] = result2[3]/100\n",
        "    accuracy[2][i] = result2[5]"
      ],
      "metadata": {
        "id": "yHHWCYL0L8db",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 3 Candidate Models Performance Comparison Table\n",
        "\n",
        "accuracy_mean1 = np.mean(accuracy,axis = 1)\n",
        "max_acc1 = np.max(accuracy,axis = 1)\n",
        "min_acc1 = np.min(accuracy,axis = 1)\n",
        "acc_std1 = np.std(accuracy,axis = 1)\n",
        "mean_bcpower = np.mean(bcpower,axis = 1)\n",
        "mean_hcpower = np.mean(hcpower,axis = 1)\n",
        "\n",
        "compare1 = pd.DataFrame(index = ['Conv 1D Vert','Bi-LSTM','GRU'])\n",
        "compare1['Accuracy'] = np.round(accuracy_mean1,4)\n",
        "compare1['Recall BC'] = np.round(mean_bcpower,4)\n",
        "compare1['Recall HC'] = np.round(mean_hcpower,4)\n",
        "compare1['Max Accr'] = np.round(max_acc1,4)\n",
        "compare1['Min Accr'] = np.round(min_acc1,4)\n",
        "compare1['Accr Std'] = np.round(acc_std1,4)\n",
        "compare1"
      ],
      "metadata": {
        "id": "POuJ5MtMlPja",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Reference**\n",
        "[1] Juan Pablo Bonfrisco Ayala, Javier de Vicente Moreno. (2019). *APPLYING DEEP LEARNING TECHNIQUES TO PREDICT THE BANKRUPTCY OF AN ENTERPRISE*"
      ],
      "metadata": {
        "id": "UddOQ6xSgiuN"
      }
    }
  ]
}